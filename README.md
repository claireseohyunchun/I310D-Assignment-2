# I310D-Assignment-2
The first thing I did in order to test the Perspective model for bias was setting the thresholds. Also, I decided to evaluate the toxic and severe_toxic labels to set the thresholds. The thresholds I came up with were if the score was higher than 0.5, the comment is considered toxic. If the score was higher than 0.75, the comment is considered severely toxic. The reason why I chose these thresholds is because while discovering the excel sheet, I found out that the comments that got a severe_toxic label were likely to score greater than 0.75.

While exploring the dataset, I found out that the comments that contain cursing and words that are racist had a high score of toxicity and likely to have toxic and severe_toxic labels. However, there were some cases where comments that don’t seem offensive at all received high scores. 

I put some of those comments in the code and ran to see if the actual toxicity score differs from what it says on the excel sheet. For example, the comment on row 9718, it has a toxicity score of 0.7910531 even though the comment is “I’m sorry.” It also received toxic, insult, and identity_hate labels which are wrong. When I put the comment “I’m sorry” in the code in Jupyter, it only received a score of 0.023288755. 

Also, what I found curious was even though the comments do not contain any offensive word, it was considered toxic/severe toxic. The comment on row 9003 as an example, has a toxicity score of 0.8595876 even though it only contains “””. I wanted to put “”” in the code and see the actual toxicity score of it, but Jupyter didn’t let me put “”” in the code. 

The hypothesis I made was that the comments containing the word “white” will have a higher score of toxicity. I decided to test the perspective model for bias and test if my hypothesis is true or false by putting the word “white” in the search box and see if those comments are likely to have higher scores of toxicity. I also explored the dataset and if the comments with the word “white” had high toxicity score, I put them in my code to check if they actually scored a high toxicity. 

These are the processes of finding whether my hypothesis is true or false. I read over the significant amount of comments that contain the word “white” and have high toxicity scores and determined if they have racism in the comment. I also put those comments in the Jupyter code to see if its actual score was high as well. I employed five example comments that have high scores to prove that my hypothesis is true. Those five comments are selected based on its high toxicity scores, and I found out that most of the comments with the word “white” in it contained racism through exploring the dataset. (Of course I explored a lot more than five, but I believe that these five are the best examples to support my hypothesis.)

One TP and one FN I found were: the comment “:::::A:s I mentioned, your facts mean nothing in THIS case. But yeah, keep holding on to your beliefs that black men are rapists who just have to rape white women because they can't help themselves (loud snickering) and that white men are the gods of this earth, and that black women are nothing but liars. Whatever makes you feel better." represents the case of TP since it supports the hypothesis and the reason why it received a high toxicity score is because of its racist aspect. The comment “A white fatass that ain't sh!t and acting like a bihcpadding” represents the case of FP since it does not contain any racist content, but it has a high toxicity score. This comment was marked with a toxic label because of its offensiveness, not racism. Since the comments that have the word “white” in it likely contain racism which can be considered as toxic, severe_toxic, obscene, threat, insult, and identity_hate, I concluded that my hypothesis is true. 
